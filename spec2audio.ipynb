{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import datasets\n",
    "\n",
    "TARGET_SR = 44100\n",
    "\n",
    "def audio2mel(filepath: str, start: int = 0):\n",
    "    #TODO load different parts of the audio, not just the beginning\n",
    "    x, sr = librosa.load(filepath, sr=TARGET_SR, mono=True)\n",
    "    start, end = 0, 432*512-1\n",
    "\n",
    "    stft = np.abs(librosa.stft(x[start:end], n_fft=2048, hop_length=512))\n",
    "    mel = librosa.feature.melspectrogram(sr=sr, S=stft**2, n_mels=128)\n",
    "    log_mel = librosa.amplitude_to_db(mel)\n",
    "\n",
    "    return log_mel\n",
    "\n",
    "def make_dataset(files_dir: str, *, count: int = -1):\n",
    "    all_mels = {}\n",
    "    loaded_count = 0\n",
    "    files = os.listdir(files_dir)\n",
    "    for file in tqdm(files, desc=\"Processing files...\", total=len(files) if count == -1 else min(count, len(files))):\n",
    "        if not file.lower().endswith('.mp3') and not file.lower().endswith(\".wav\"):  # Adds wav support\n",
    "            continue\n",
    "        if count > 0 and loaded_count >= count:\n",
    "            break\n",
    "        filepath = os.path.join(files_dir, file)\n",
    "        filename = os.path.basename(filepath)\n",
    "\n",
    "        # Call the function on the MP3 file\n",
    "        try:\n",
    "            mel, sr = audio2mel(filepath)\n",
    "            if mel.shape == (128, 432):\n",
    "                all_mels[filename + f'_{sr}'] = mel\n",
    "                loaded_count += 1\n",
    "            else:\n",
    "                print(\"Skipping shape {}\".format(mel.shape))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    return all_mels\n",
    "\n",
    "def save_spec(dataset: dict, save_dir: str):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    np.savez_compressed(os.path.join(save_dir, \"specs.npz\"), **dataset)\n",
    "\n",
    "\n",
    "def _check_spec(spec):\n",
    "    # To keep my sanity\n",
    "    assert spec.shape == (128, 432), f\"Shape is {spec.shape}, expected (128, 432)\"\n",
    "    assert spec.dtype == np.float32, f\"Data type is {spec.dtype}, expected np.float32\"\n",
    "    assert np.isfinite(spec).all(), \"Data contains non-finite values\"\n",
    "    assert np.abs(spec).max() <= 80, \"Data contains values greater than 80 dB\"\n",
    "    assert np.abs(spec).max() > 1, \"Empty data, or you probably forget to unnormalize it\"\n",
    "\n",
    "def load_spec(spec_file: str):\n",
    "    ds = np.load(spec_file)\n",
    "    dsdict = {}\n",
    "    for key in ds:\n",
    "        try:\n",
    "            _check_spec(ds[key])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {key}: {e}\")\n",
    "            continue\n",
    "        dsdict[key] = ds[key] / 80.0 # Normalize to [-1, 1]\n",
    "    return dsdict\n",
    "\n",
    "def mel_to_audio(spec, sr: int, n_iter: int = 32):\n",
    "    _check_spec(spec)\n",
    "    mel = librosa.db_to_amplitude(spec * 80.0)\n",
    "\n",
    "    mel_basis = librosa.filters.mel(sr, n_fft=2048, n_mels=128)\n",
    "    inv_mel_basis = np.linalg.pinv(mel_basis)\n",
    "    stft_magnitude = np.dot(inv_mel_basis, mel)\n",
    "\n",
    "    stft_magnitude_squared = stft_magnitude**2\n",
    "    audio = librosa.griffinlim(stft_magnitude_squared, hop_length=512, n_iter=n_iter)\n",
    "\n",
    "    return audio\n",
    "\n",
    "def convert_ds_to_hf_dataset(ds: dict, test_size: float = 0.1):\n",
    "    hfds_dict = {\"filename\": [], \"mel\": []}\n",
    "    for key in ds:\n",
    "        hfds_dict[\"filename\"].append(key)\n",
    "        hfds_dict[\"mel\"].append(ds[key])\n",
    "    hf_ds = datasets.Dataset.from_dict(hfds_dict)\n",
    "\n",
    "    #Train test split\n",
    "    train_ds, test_ds = hf_ds.train_test_split(test_size=test_size)\n",
    "    return hf_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "# huggingface_hub.login(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = convert_ds_to_hf_dataset(load_spec(\"./output/specs.npz\"))\n",
    "\n",
    "# Upload to hf hub\n",
    "ds.push_to_hub(\"mel-spectrogram-dataset-test\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Configure the model\n",
    "model = UNet2DModel(\n",
    "    sample_size=(128, 432),\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(32, 64, 64),\n",
    "    down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\")\n",
    ").to(device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Load the dataset\n",
    "#TODO replace this line or smth\n",
    "dataset = load_dataset(\"mel-spectrogram-dataset-test\")\n",
    "\n",
    "# DataLoader expects a torch.Tensor, thus a conversion function is needed\n",
    "def collate_fn(batch):\n",
    "    mels = [item['mel'].unsqueeze(0) for item in batch]  # Adding channel dimension\n",
    "    mels = torch.stack(mels).to(device)  # Shape will be [batch_size, 1, 128, 432]\n",
    "    return mels\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Scheduler and Optimizer\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add noise\n",
    "        noise = torch.randn_like(batch)\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=device, dtype=torch.int64)\n",
    "        noisy_batch = noise_scheduler.add_noise(batch, noise, timesteps)\n",
    "\n",
    "        # Model forward pass\n",
    "        noise_pred = model(noisy_batch, timesteps)[0]\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = loss_func(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} completed, Average Loss: {epoch_loss / len(train_loader)}')\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "# from torchvision.utils import make_grid\n",
    "# from diffusers import DDPMScheduler, UNet2DModel\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "# model = UNet2DModel(\n",
    "#     sample_size=(128, 432), # Dimensions must be a multiple of 2 ** (len(block_out_channels) - 1) = 4.\n",
    "#     in_channels=1,\n",
    "#     out_channels=1,\n",
    "#     layers_per_block=2,\n",
    "#     block_out_channels=(32, 64, 64),\n",
    "#     down_block_types=(\n",
    "#         \"DownBlock2D\",\n",
    "#         \"AttnDownBlock2D\",\n",
    "#         \"DownBlock2D\",\n",
    "#     ),\n",
    "#     up_block_types=(\n",
    "#         \"UpBlock2D\",\n",
    "#         \"AttnUpBlock2D\",\n",
    "#         \"UpBlock2D\",\n",
    "#     )\n",
    "# ).to(device)\n",
    "\n",
    "# batch_size = 1\n",
    "\n",
    "# noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "# opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# x = torch.randn(1, 1, 128, 432).to(device)\n",
    "\n",
    "# noise = torch.randn(x.shape, device=device)\n",
    "# timesteps = torch.randint(\n",
    "#     0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=device, dtype=torch.int64\n",
    "# ).to(device)\n",
    "# noisy_x = noise_scheduler.add_noise(x, noise, timesteps).to(device)\n",
    "# noise_pred = model(noisy_x, timesteps)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
